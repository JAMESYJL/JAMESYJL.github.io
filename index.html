<!DOCTYPE HTML>

<style>
  #full {
    display: none;
  }
  </style>

<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Junliang Ye</title>
  
  <meta name="author" content="Junliang Ye">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>


<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Junliang Ye | 叶俊良</name>
              </p>
              <p> 
                I am a first-year master's student in the Department of <a href="https://www.cs.tsinghua.edu.cn/index.htm"> 
                  Computer Science </a> at <a href="https://www.tsinghua.edu.cn/en/"> Tsinghua University </a>, 
                  advised by Prof. <a href="https://ml.cs.tsinghua.edu.cn/~jun/index.shtml">Jun Zhu</a>. 
                  In 2022, I obtained my B.S. in the School of Mathematical Sciences at Peking University.
              </p>
              <p>
                My research interest lies in the <b>AI4science</b>, <b>machine learning</b> and <b>AIGC</b>. 
              </p>
              <p style="text-align:center">
                <a href="mailto:yejl23@mails.tsinghua.edu.cn">Email</a> &nbsp/&nbsp
                <a href="https://jamesyjl.github.io/">CV</a> &nbsp/&nbsp
                <a href="jamesyjl.github.io"> Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/JAMESYJL"> Github </a>
              </p>
            </td>
            <td style="padding:3%;width:40%;max-width:40%">
              <img style="width:70%;max-width:70%" alt="profile photo" src="images/junliang1.jpg" class="hoverZoomLink">
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p><heading>Publications</heading></p>
              <p>
                * indicates equal contribution
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/pipeline.jpg" alt="dise">
            </td>
            <td width="80%" valign="center">
              <papertitle>DreamReward: Aligning Human Preference in Text-to-3D Generation</papertitle>
              <br>
              <strong>Junliang Ye*</strong>, 
              Fangfu Liu*, 
              Qixiu Li,
              Zhengyi Wang,
              Yikai Wang,
              Xinzhou Wang,
              <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>,
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              Jun Zhu
              <br>
              <em>Arxiv, 2024</em>
              <br>
              <a >[arXiv]</a>
              <a >[Code]</a>
              <a >[Project Page]</a> 
              <br>
              <p> we present a comprehensive framework, coined DreamReward, to 
                learn and improve text-to-3D models from human preference feedback.</p>
            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/pipeline2.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>AnimatableDreamer: Text-Guided Non-rigid 3D Model Generation 
                and Reconstruction with Canonical Score Distillation</papertitle>
              <br>
              Xinzhou Wang,
              Yikai Wang,
              <strong>Junliang Ye</strong>, 
              Zhengyi Wang,
              Fuchun Sun,
              Pengkun Liu,
              Ling Wang,
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
              Kai Sun, 
              Xintong Wang, 
              Bin He
              <br>
              <em>Arxiv, 2023</em>
              <br>
              <a href="https://arxiv.org/abs/2312.03795">[arXiv]</a>
              <a href="https://github.com/AnimatableDreamer/AnimatableDreamer">[Code]</a>
              <a href="https://animatabledreamer.github.io/">[Project Page]</a> 
              <br>
              <p> We propose Sherpa3D, a new text-to-3D framework that achieves high-fidelity, generalizability, and geometric consistency simultaneously. Extensive experiments show the superiority of our Sherpa3D over the state-of-the-art text-to-3D methods in terms of quality and 3D consistency.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/pipeline3-2.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>PKU_WICT at TRECVID 2022: Disaster Scene Description and Indexing Task</papertitle>
              <br>
              Yanzhe Chen, 
              HsiaoYuan Hsu,
              <strong>Junliang Ye</strong>, 
              Zhiwen Yang, 
              Zishuo Wang, 
              Xiangteng He, 
              Yuxin Peng
              <br>
              Virtual, Online
              <br>
              <a >[arXiv]</a>
              <a >[Code]</a>
              <a >[Project Page]</a> 
              <br>
              <p> We achieved first place in the TRECVID 2022 competition.</p>
            </td>
          </tr>

        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <li style="margin: 5px;"> 
                Review for <b>CVPR 2023</b>.
              </li>
          </td>
        </tr>
      </tbody></table>
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>

<p><center>
	  <div id="clustrmaps-widget" style="width:5%">
      <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=B_hoqUcZkAVteexiuKv_tIvNw9enA1g2tIC3ypxXP2E"></script>
	  <br>
	    &copy; Junliang Ye | Last updated: 18 Mar, 2024
</center></p>
</body>

</html>
